# SuperSBT: Faster and Stronger Single Branch Transformer Tracking

Existing transformer-based trackers which are built on two popular tracking paradigms, \ie Siamese-based and DCF-based tracking, mostly leverage transformer model to solely focus on one of these three issues: feature extraction, feature enhancement or feature fusion. Differently, this work presents a novel tracking architecture on top of Single Branch Transformer (SBT). Through two crucial modifications for tracking problem, \ie dynamic feature extraction and correspondence establishment, our conceptually neat tracking framework which is named SuperSBT, simultaneously addresses above three issues. Specifically, with dedicated non-parametric attention operator design, SuperSBT can extract target-dependent features as well as building comprehensive interactions between target and search area, while achieving high inference speed and marvelous tracking performance. We further conduct extensive investigations on the crucial design and architecture variants to provide more insights for SBT tracking. Through our experiments, SuperSBT sets a new record with xxx SUC on LaSOT, surpassing STARK by xx while still running at 45 FPS. Besides, it achieves state of-the-art performances with xxx SUC, xxx SUC and xxx AO on other challenging LaSOText, TrackingNet, and GOT-10k datasets.
